{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng_8Tvgcvp4M"
      },
      "source": [
        "# 🧬 NeuroGenAI | DNABERT Semantic Embeddings Bridge\n",
        "### What is DNABERT?\n",
        "#### DNABERT is a transformer-based model pre-trained on genomic sequences using k-mer tokenization. Like BERT for language, it captures semantic patterns in DNA.\n",
        "\n",
        "### Why k-mer Encoding?\n",
        "#### DNA is tokenized into overlapping sequences (e.g., \"ACGTGA\"). This allows the model to learn motifs and structures.\n",
        "\n",
        "### Why LoRA / QLoRA?\n",
        "#### PEFT methods like LoRA enable fast, low-resource fine-tuning. Great for adapting DNABERT to specific genomes or classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0_Y_gT5vq-F"
      },
      "source": [
        "## 🔧 Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9djSLLYXu3ux"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src path\n",
        "import sys\n",
        "src_path = Path().resolve().parents[1] / \"src\"\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.append(str(src_path))\n",
        "\n",
        "from nlp.dna_embedding_model import DNAEmbedder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dAR0Gihvt1r"
      },
      "source": [
        "## 📥 Load Clean FASTA Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RDxxL_nu-St"
      },
      "outputs": [],
      "source": [
        "# Load cleaned FASTA sequences\n",
        "fasta_path = \"data/processed/human_fasta_clean.csv\"\n",
        "df = pd.read_csv(fasta_path)\n",
        "\n",
        "# For test: use only first N\n",
        "df = df[df['Length'] >= 30].head(100)  # Change as needed\n",
        "print(f\"✅ Loaded {len(df)} sequences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmEHrNrYvw6M"
      },
      "source": [
        "## 🧠 Initialize DNABERT Embedding Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRh1swpQvAGD"
      },
      "outputs": [],
      "source": [
        "embedder = DNAEmbedder(model_id=\"armheb/DNA_bert_6\", k=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaZeRwucvyy0"
      },
      "source": [
        "## 💾 Save Embeddings as .npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5xZDEqglvEV0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔁 Embedding sequence 1/100...\n",
            "🔁 Embedding sequence 2/100...\n",
            "🔁 Embedding sequence 3/100...\n",
            "🔁 Embedding sequence 4/100...\n",
            "🔁 Embedding sequence 5/100...\n",
            "🔁 Embedding sequence 6/100...\n",
            "🔁 Embedding sequence 7/100...\n",
            "🔁 Embedding sequence 8/100...\n",
            "🔁 Embedding sequence 9/100...\n",
            "🔁 Embedding sequence 10/100...\n",
            "🔁 Embedding sequence 11/100...\n",
            "🔁 Embedding sequence 12/100...\n",
            "🔁 Embedding sequence 13/100...\n",
            "🔁 Embedding sequence 14/100...\n",
            "🔁 Embedding sequence 15/100...\n",
            "🔁 Embedding sequence 16/100...\n",
            "🔁 Embedding sequence 17/100...\n",
            "🔁 Embedding sequence 18/100...\n",
            "🔁 Embedding sequence 19/100...\n",
            "🔁 Embedding sequence 20/100...\n",
            "🔁 Embedding sequence 21/100...\n",
            "🔁 Embedding sequence 22/100...\n",
            "🔁 Embedding sequence 23/100...\n",
            "🔁 Embedding sequence 24/100...\n",
            "🔁 Embedding sequence 25/100...\n",
            "🔁 Embedding sequence 26/100...\n",
            "🔁 Embedding sequence 27/100...\n",
            "🔁 Embedding sequence 28/100...\n",
            "🔁 Embedding sequence 29/100...\n",
            "🔁 Embedding sequence 30/100...\n",
            "🔁 Embedding sequence 31/100...\n",
            "🔁 Embedding sequence 32/100...\n",
            "🔁 Embedding sequence 33/100...\n",
            "🔁 Embedding sequence 34/100...\n",
            "🔁 Embedding sequence 35/100...\n",
            "🔁 Embedding sequence 36/100...\n",
            "🔁 Embedding sequence 37/100...\n",
            "🔁 Embedding sequence 38/100...\n",
            "🔁 Embedding sequence 39/100...\n",
            "🔁 Embedding sequence 40/100...\n",
            "🔁 Embedding sequence 41/100...\n",
            "🔁 Embedding sequence 42/100...\n",
            "🔁 Embedding sequence 43/100...\n",
            "🔁 Embedding sequence 44/100...\n",
            "🔁 Embedding sequence 45/100...\n",
            "🔁 Embedding sequence 46/100...\n",
            "🔁 Embedding sequence 47/100...\n",
            "🔁 Embedding sequence 48/100...\n",
            "🔁 Embedding sequence 49/100...\n",
            "🔁 Embedding sequence 50/100...\n",
            "🔁 Embedding sequence 51/100...\n",
            "🔁 Embedding sequence 52/100...\n",
            "🔁 Embedding sequence 53/100...\n",
            "🔁 Embedding sequence 54/100...\n",
            "🔁 Embedding sequence 55/100...\n",
            "🔁 Embedding sequence 56/100...\n",
            "🔁 Embedding sequence 57/100...\n",
            "🔁 Embedding sequence 58/100...\n",
            "🔁 Embedding sequence 59/100...\n",
            "🔁 Embedding sequence 60/100...\n",
            "🔁 Embedding sequence 61/100...\n",
            "🔁 Embedding sequence 62/100...\n",
            "🔁 Embedding sequence 63/100...\n",
            "🔁 Embedding sequence 64/100...\n",
            "🔁 Embedding sequence 65/100...\n",
            "🔁 Embedding sequence 66/100...\n",
            "🔁 Embedding sequence 67/100...\n",
            "🔁 Embedding sequence 68/100...\n",
            "🔁 Embedding sequence 69/100...\n",
            "🔁 Embedding sequence 70/100...\n",
            "🔁 Embedding sequence 71/100...\n",
            "🔁 Embedding sequence 72/100...\n",
            "🔁 Embedding sequence 73/100...\n",
            "🔁 Embedding sequence 74/100...\n",
            "🔁 Embedding sequence 75/100...\n",
            "🔁 Embedding sequence 76/100...\n",
            "🔁 Embedding sequence 77/100...\n",
            "🔁 Embedding sequence 78/100...\n",
            "🔁 Embedding sequence 79/100...\n",
            "🔁 Embedding sequence 80/100...\n",
            "🔁 Embedding sequence 81/100...\n",
            "🔁 Embedding sequence 82/100...\n",
            "🔁 Embedding sequence 83/100...\n",
            "🔁 Embedding sequence 84/100...\n",
            "🔁 Embedding sequence 85/100...\n",
            "🔁 Embedding sequence 86/100...\n",
            "🔁 Embedding sequence 87/100...\n",
            "🔁 Embedding sequence 88/100...\n",
            "🔁 Embedding sequence 89/100...\n",
            "🔁 Embedding sequence 90/100...\n",
            "🔁 Embedding sequence 91/100...\n",
            "🔁 Embedding sequence 92/100...\n",
            "🔁 Embedding sequence 93/100...\n",
            "🔁 Embedding sequence 94/100...\n",
            "🔁 Embedding sequence 95/100...\n",
            "🔁 Embedding sequence 96/100...\n",
            "🔁 Embedding sequence 97/100...\n",
            "🔁 Embedding sequence 98/100...\n",
            "🔁 Embedding sequence 99/100...\n",
            "🔁 Embedding sequence 100/100...\n",
            "✅ Final embedding shape: (100, 768)\n",
            "📁 Saved to: data/processed/fasta_dnabert_embeddings.npy\n"
          ]
        }
      ],
      "source": [
        "# Extract sequences\n",
        "sequences = df[\"Sequence\"].tolist()\n",
        "\n",
        "# Embed all\n",
        "embeddings = embedder.embed_batch(sequences)\n",
        "print(\"✅ Final embedding shape:\", embeddings.shape)\n",
        "\n",
        "# Save as .npy\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "np.save(\"data/processed/fasta_dnabert_embeddings.npy\", embeddings)\n",
        "print(\"📁 Saved to: data/processed/fasta_dnabert_embeddings.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySfZpxabv1TD"
      },
      "source": [
        "## 🧾 Log Embedding Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_O-DIvFvHZb"
      },
      "outputs": [],
      "source": [
        "# Save metadata for reproducibility\n",
        "meta = {\n",
        "    \"model_id\": embedder.model_id,\n",
        "    \"vector_dim\": embeddings.shape[1],\n",
        "    \"sequence_count\": embeddings.shape[0],\n",
        "    \"source_fasta\": fasta_path,\n",
        "    \"kmer_size\": embedder.k,\n",
        "    \"device\": embedder.device,\n",
        "    \"huggingface_url\": f\"https://huggingface.co/{embedder.model_id}\"\n",
        "}\n",
        "\n",
        "with open(\"data/outputs/3. DNABERT + SNN + NLP/embedding_info.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=4)\n",
        "\n",
        "print(\"✅ Metadata saved to: data/outputs/3. DNABERT + SNN + NLP/embedding_info.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
